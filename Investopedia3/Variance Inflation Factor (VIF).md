---
alias: [Variance Inflation Factor (VIF)]
created: 2021-03-03T00:36:24 (UTC +11:00)
tags: [Variance Inflation Factor (VIF) Definition, Variance Inflation Factor (VIF)]
source: https://www.investopedia.com/terms/v/variance-inflation-factor.asp
author: Investopedia Staff
---

# Variance Inflation Factor (VIF) Definition

> ## Excerpt
> Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables.

---

Variance Inflation Factor (VIF)
## What Is a Variance Inflation Factor (VIF)?

Variance inflation factor (VIF) is a measure of the amount of [multicollinearity](https://www.investopedia.com/terms/m/multicollinearity.asp) in a set of multiple [regression](https://www.investopedia.com/terms/r/regression.asp) variables. Mathematically, the VIF for a regression model variable is equal to the ratio of the overall model [variance](https://www.investopedia.com/terms/v/variance.asp) to the variance of a model that includes only that single independent variable. This ratio is calculated for each independent variable. A high VIF indicates that the associated independent variable is highly collinear with the other variables in the model.

### Key Takeaways

-   A variance inflation factor (VIF) provides a measure of multicollinearity among the independent variables in a multiple regression model.
-   Detecting multicollinearity is important because while multicollinearity does not reduce the explanatory power of the model, it does reduce the statistical significance of the independent variables. 
-   A large variance inflation factor (VIF) on an independent variable indicates a highly collinear relationship to the other variables that should be considered or adjusted for in the structure of the model and selection of independent variables.

## Understanding a Variance Inflation Factor (VIF)

A variance inflation factor is a tool to help identify the degree of multicollinearity. A multiple regression is used when a person wants to test the effect of multiple variables on a particular outcome. The dependent variable is the outcome that is being acted upon by the independent variables—the inputs into the model. Multicollinearity exists when there is a linear relationship, or correlation, between one or more of the independent variables or inputs.

Multicollinearity creates a problem in the multiple regression because the inputs are all influencing each other. Therefore, they are not actually independent, and it is difficult to test how much the combination of the independent variables affects the dependent variable, or outcome, within the regression model. In statistical terms, a multiple regression model where there is high multicollinearity will make it more difficult to estimate the relationship between each of the independent variables and the dependent variable. Small changes in the data used or in the structure of the model equation can produce large and erratic changes in the estimated coefficients on the independent variables.

To ensure the model is properly specified and functioning correctly, there are tests that can be run for multicollinearity. Variance inflation factor is one such measuring tool. Using variance inflation factors helps to identify the severity of any multicollinearity issues so that the model can be adjusted. Variance inflation factor measures how much the behavior (variance) of an independent variable is influenced, or inflated, by its interaction/correlation with the other independent variables. Variance inflation factors allow a quick measure of how much a variable is contributing to the [standard error](https://www.investopedia.com/terms/s/standard-error.asp) in the regression. When significant multicollinearity issues exist, the variance inflation factor will be very large for the variables involved. After these variables are identified, several approaches can be used to eliminate or combine collinear variables, resolving the multicollinearity issue.

## Special Considerations

### Multicollinearity

While multicollinearity does not reduce a model's overall predictive power, it can produce estimates of the regression coefficients that are not statistically significant. In a sense, it can be thought of as a kind of double-counting in the model. When two or more independent variables are closely related or measure almost the same thing, then the underlying effect that they measure is being accounted for twice (or more) across the variables. It becomes difficult or impossible to say which variable is really influencing the independent variable. This is a problem because the goal of many [econometric](https://www.investopedia.com/terms/e/econometrics.asp) models is to test exactly this sort of statistical relationship between the independent variables and the dependent variable.

For example, suppose that an economist wants to test whether there is a statistically significant relationship between the unemployment rate (independent variable) and the inflation rate (dependent variable). Including additional independent variables that are related to the [unemployment rate](https://www.investopedia.com/terms/u/unemploymentrate.asp), such a new initial [jobless claims](https://www.investopedia.com/terms/j/jobless-claims.asp), would be likely to introduce multicollinearity into the model. The overall model might show strong, statistically sufficient explanatory power, but be unable to identify if the effect is mostly due to the unemployment rate or to the new initial jobless claims. This is what the VIF would detect, and it would suggest possibly dropping one of the variables out of the model or finding some way to consolidate them to capture their joint effect depending on what specific hypothesis the researcher is interested in testing.
